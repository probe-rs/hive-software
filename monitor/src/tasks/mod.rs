//! Handles all tasks which can be triggered by external users.
//! The Tasks which are handled by the task manager are test tasks which can be triggered by using the test endpoint of the webserver as well as hardware reinitialization tasks which can be triggered by the backend graphql api.
//!
//! The way Hive is built is that it can only process one test or hardware reinitialization request at a time on the hardware.
//! Therefore the [`TaskManager`] acts as a task buffer and queue which decides when and which tasks are executed on the hardware.
//! It has different queue and buffer mechanisms depending on the task type:
//!
//! ## Hardware reinitialization task
//! The reinitialization task is simply handled by using a [`mpsc`] channel with a buffer of one task.
//! This means that if a user requests a hardware reinitialization while another hardware reinitialization task is still in the queue the new task will get rejected and the user informed.
//! The rationale behind this is that it does not make sense to queue hardware reinitializations as the queued one will take over any changes in the DB from either user which triggered a hardware reinitialization.
//!
//! ## Test task
//! Test task management is more complex. To understand what the task manager is doing with a test task we need to understand how a test task is initiated and handled on the webserver:
//! ```text
//! Requesting user                                   Monitor                   
//! +------------------+   POST multipart request     +-----------------------+  
//! |Test Task:        | ---------------------------> |Receive Test task      |  
//! | - Runner binary  |                              | - Generate Ws ticket  |  
//! | - Other options  |   Response with WS ticket    | - Store in test cache |  
//! |                  | <--------------------------- |                       |  
//! +------------------+                              +-----------------------+  
//!                                                                              
//! +------------------+                              +-----------------------+  
//! |Prepare WS        |  GET Ws upgrade + WS ticket  |Receive WS upgrade req |  
//! | - Append received| ---------------------------> | - Check if ticket is  |  
//! |   WS ticket to   |                              |   valid and not timed |  
//! |   Request        |    open websocket if valid   |   out                 |  
//! |                  | <--------------------------- | - Move task into valid|  
//! |                  |                              |   queue if ticket is  |  
//! |                  |                              |   valid               |  
//! +------------------+                              +-----------------------+  
//!                                                                              
//! +------------------+                              +-----------------------+  
//! |WS communication  |         bidirectional        |WS communication       |  
//! | - Receive status | <--------------------------> | - Send status and test|  
//! |   and test       |                              |   results             |  
//! |   results        |                              | - Close WS            |  
//! +------------------+                              +-----------------------+
//! ```
//! As seen in the diagram above the initial test request is done via a REST API. In this request the runner binary is supplied as well as configuration data for the test (This data is received by the [`TaskManager`] as [`TestTask`] struct).
//! This test is stored in the initial `test_cache` of the [`TaskManager`] before the server generates a random [`WsTicket`] and sends it back to the requesting user in a response.
//!
//! Now the user is required to initiate a websocket request. The websocket is used to communicate test status messages as well as the final test results from the monitor to the runner.
//! The reason on why a websocket is used is outlined in the [`crate::webserver`] documentation.
//!
//! Now what has a [`WsTicket`] to do with this? Basically this ticket is a base64 encoded random sequence of bytes generated by the monitor.
//! As we do not want any random users to be able to open websockets on the monitor we supply the test request user this [`WsTicket`] which the user then can use to successfully open a websocket.
//! This way only users who have previously sent a valid test request can later successfully open a websocket.
//!
//! In order to track which task in the `test_cache` corresponds to which [`WsTicket`] the `test_cache` is a key value pair cache where the ticket is used as a key. For security purposes the `test_cache` is a timed cache,
//! which means that every value placed inside it will expire after the [`WS_CONNECT_TIMEOUT_SECS`] has elapsed. This ensures that an issued ticket cannot be used forever to open a websocket.
//!
//! In case the user does not manage to open a websocket with the received [`WsTicket`] the previously sent task is dropped in the cache and the user needs to start anew by sending another test request to the monitor.
//!
//! If the user sends a websocket upgrade request with a ticket the task manager searches for the ticket in the `test_cache`. If it is valid the websocket is opened and the corresponding [`TestTask`] is moved from the `test_cache` into
//! the `valid_test_queue` [`mpsc`] channel where it waits for execution.
//!
//! The initial `test_cache` also has a limit of tasks which it can store which is defined by the [`TASK_CACHE_LIMIT`]. In case any test requests are done while this cache is full they are rejected.
//!
//! # [`TaskManager`] vs [`TaskScheduler`]
//! It is important to note that the [`TaskManager`] does not run any tasks. It only manages and queues them for the [`TaskScheduler`] which executes the tasks and reports the results.
use std::sync::Arc;
use std::time::Duration;

use axum::response::IntoResponse;
use cached::Cached;
use cached::stores::TimedCache;
use comm_types::test::TaskRunnerMessage;
use hyper::StatusCode;
use thiserror::Error;
use tokio::runtime::Runtime;
use tokio::sync::Mutex as AsyncMutex;
use tokio::sync::mpsc::{self, Receiver as MpscReceiver, Sender as MpscSender};

use crate::database::MonitorDb;

use self::reinit_task::ReinitializationTask;
use self::scheduler::TaskScheduler;
use self::test_task::TestTask;
use self::ws::WsTicket;

pub mod reinit_task;
pub mod scheduler;
pub mod test_task;
mod util;
pub mod ws;

/// How many tasks can wait for a WS connection simultaneously
const TASK_CACHE_LIMIT: usize = 10;
/// Duration until a cached test request is invalidated if no websocket for the corresponding [`TestTask`] has been created
pub const WS_CONNECT_TIMEOUT_SECS: u64 = 30;

/// Task trait
pub trait Task {
    /// Runs the task
    fn run(self, runner: &mut TaskScheduler);
}

/// The possible task types the [`TaskManager`] can handle
pub enum TaskType {
    TestTask(TestTask),
    ReinitTask(ReinitializationTask),
    Shutdown,
}

#[derive(Debug, Error)]
pub enum TaskManagerError {
    #[error("The test queue is full. Please try again later.")]
    TestQueueFull,
    #[error(
        "Discarded this reinitialization task as another reinitialization task is still waiting for execution"
    )]
    ReinitTaskDiscarded,
    #[error(
        "The provided ticket is invalid or the client took too long to connect the websocket after the initial test request"
    )]
    TestTaskTicketInvalid,
}

impl IntoResponse for TaskManagerError {
    fn into_response(self) -> axum::response::Response {
        match self {
            TaskManagerError::TestQueueFull => {
                (StatusCode::SERVICE_UNAVAILABLE, self.to_string()).into_response()
            }
            TaskManagerError::ReinitTaskDiscarded => {
                (StatusCode::CONFLICT, self.to_string()).into_response()
            }
            TaskManagerError::TestTaskTicketInvalid => {
                (StatusCode::UNAUTHORIZED, self.to_string()).into_response()
            }
        }
    }
}

/// Manages all incoming tasks. For more info take a look at the module documentation.
pub struct TaskManager {
    reinit_task_sender: MpscSender<ReinitializationTask>,
    /// The initial cache which contains all valid test requests which do not yet have a websocket connection
    test_cache: AsyncMutex<TimedCache<WsTicket, TestTask>>,
    // Test queue which contains all test that do have a valid websocket connection and are ready for testing
    valid_test_task_sender: MpscSender<TestTask>,
}

impl TaskManager {
    /// Creates a [`TaskManager`] and returns its corresponding [`TaskScheduler`] which can then be started by the application
    pub fn new(db: Arc<MonitorDb>, runtime: Arc<Runtime>) -> (Arc<Self>, TaskScheduler) {
        let (valid_test_task_sender, valid_test_task_receiver) = mpsc::channel(TASK_CACHE_LIMIT);
        let (reinit_task_sender, reinit_task_receiver) = mpsc::channel(1);

        (
            Arc::new(Self {
                reinit_task_sender,
                test_cache: AsyncMutex::new(TimedCache::with_lifespan_and_capacity(
                    Duration::from_secs(WS_CONNECT_TIMEOUT_SECS),
                    TASK_CACHE_LIMIT,
                )),
                valid_test_task_sender,
            }),
            TaskScheduler::new(valid_test_task_receiver, reinit_task_receiver, runtime, db),
        )
    }

    /// Attempts to register a new [`TestTask`] scheduled for execution. If successful, a [`WsTicket`] is returned, which should be sent back to the client
    /// so the client can reopen a websocket with said ticket to receive the test status and results.
    ///
    /// This function can fail in case the internal task queue has reached the [`TASK_CACHE_LIMIT`].
    pub async fn register_test_task(
        &self,
        mut task: TestTask,
    ) -> Result<WsTicket, TaskManagerError> {
        let mut test_cache = self.test_cache.lock().await;

        if test_cache.get_store().len() >= TASK_CACHE_LIMIT {
            return Err(TaskManagerError::TestQueueFull);
        }

        let ticket = task.generate_ws_ticket();

        test_cache.cache_set(ticket.clone(), task);

        Ok(ticket)
    }

    /// Attempts to validate the provided [`WsTicket`]. If validation succeeds the corresponding [`TestTask`] is moved from the `test_cache` into the end of the `valid_test_queue` where it will be processed by the [`TaskScheduler`].
    /// During processing the [`TaskScheduler`] will send status messages and ultimately the test result to the Receiver which is returned by this function.
    ///
    /// This function can fail in case the client took longer than [`WS_CONNECT_TIMEOUT_SECS`] to connect the websocket after the test run request has been received.
    pub async fn validate_test_task_ticket(
        &self,
        ws_ticket: WsTicket,
    ) -> Result<MpscReceiver<TaskRunnerMessage>, TaskManagerError> {
        let mut test_cache = self.test_cache.lock().await;

        let mut test_task = test_cache
            .cache_remove(&ws_ticket)
            .ok_or(TaskManagerError::TestTaskTicketInvalid)?;

        drop(test_cache);

        let (sender, receiver) = mpsc::channel(5);

        test_task.insert_status_and_result_sender(sender);

        self.valid_test_task_sender
            .try_send(test_task)
            .map_err(|err| match err {
                mpsc::error::TrySendError::Full(_) => TaskManagerError::TestQueueFull,
                mpsc::error::TrySendError::Closed(_) => unreachable!(),
            })?;

        Ok(receiver)
    }

    /// Register a new reinitialization task
    ///
    /// In case a previous reinitialization task is still in queue the task which is being registered is discarded in favor of the already waiting task
    pub async fn register_reinit_task(&self, task: ReinitializationTask) {
        if let Err(err) = self.reinit_task_sender.try_send(task) {
            match err {
                mpsc::error::TrySendError::Full(task) => {
                    let _ = task
                        .task_complete_sender
                        .send(Err(TaskManagerError::ReinitTaskDiscarded));
                }
                mpsc::error::TrySendError::Closed(_) => unreachable!(),
            }
        }
    }
}
